{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae634e02-3166-4aa1-a979-a37199d74561",
      "metadata": {
        "id": "ae634e02-3166-4aa1-a979-a37199d74561"
      },
      "source": [
        "# IndabaX HACKATHON"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec6c71aa-0355-4f69-9fec-b15f1b573c32",
      "metadata": {
        "id": "ec6c71aa-0355-4f69-9fec-b15f1b573c32"
      },
      "source": [
        "![IMS_Logo.png](attachment:5036202f-d31d-4fb2-80c0-2df537859253.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bd63ee-4f19-4922-96d8-f99bc1086464",
      "metadata": {
        "id": "e3bd63ee-4f19-4922-96d8-f99bc1086464"
      },
      "source": [
        "Author: Francois Naude"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c1369c-5f56-45e4-a0d5-23a357727dfc",
      "metadata": {
        "id": "95c1369c-5f56-45e4-a0d5-23a357727dfc"
      },
      "source": [
        "Contact: francois.naude@imseismology.org"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e7198ed-e0a5-4a9d-ae59-b471ead1a9fc",
      "metadata": {
        "id": "5e7198ed-e0a5-4a9d-ae59-b471ead1a9fc"
      },
      "source": [
        "In this guided hackathon you will implement and tune a model that is critical to the safety of mine workers and also helps keep mines running. You will try to produce a model that is able to give seismologists (people who analyse seismic events, like earthquakes) all the information they need to locate the origin and size of seismic events so that they can send rescue teams or give warnings of unsafe areas. Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffbd1e12-32a8-466e-b717-7a1662f424f0",
      "metadata": {
        "id": "ffbd1e12-32a8-466e-b717-7a1662f424f0"
      },
      "outputs": [],
      "source": [
        "### Get the packages to equip you for building and testing models\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import gzip\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import KLDivergence\n",
        "\n",
        "### Fetch support functions to feed our model\n",
        "%run ./Utils.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d1f5e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Auto-detect environment and setup paths\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Detect if running in Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "    # Mount Google Drive automatically\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Set Colab paths\n",
        "    BASE_DIR = \"/content/drive/MyDrive/HacakthonIMS/\"\n",
        "    TRAIN_DIR = BASE_DIR + \"trailTrain/\"\n",
        "    VAL_DIR = BASE_DIR + \"Validate1000/\"\n",
        "    TEST_DIR = BASE_DIR + \"Test1000/\"\n",
        "    CHALLENGE_DIR = BASE_DIR + \"Challenge/\"\n",
        "    \n",
        "    # Set model save directory\n",
        "    MODEL_DIR = \"/content/drive/MyDrive/HacakthonIMS/models/\"\n",
        "    \n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    # Set local paths (Windows)\n",
        "    BASE_DIR = \"c:\\\\Users\\\\Admin\\\\OneDrive - Durban University of Technology\\\\Desktop\\\\HackathonIMS\\\\\"\n",
        "    TRAIN_DIR = BASE_DIR + \"trailTrain\\\\\"\n",
        "    VAL_DIR = BASE_DIR + \"Validate1000\\\\\"\n",
        "    TEST_DIR = BASE_DIR + \"Test1000\\\\\"\n",
        "    CHALLENGE_DIR = BASE_DIR + \"Challenge\\\\\"\n",
        "    \n",
        "    # Set model save directory\n",
        "    MODEL_DIR = BASE_DIR + \"models\\\\\"\n",
        "\n",
        "# Create model directory if it doesn't exist\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Define model checkpoint path\n",
        "MODEL_CHECKPOINT_PATH = os.path.join(MODEL_DIR, \"model_checkpoint.keras\")\n",
        "HISTORY_PATH = os.path.join(MODEL_DIR, \"training_history.pkl\")\n",
        "\n",
        "print(f\"Training directory: {TRAIN_DIR}\")\n",
        "print(f\"Model checkpoint path: {MODEL_CHECKPOINT_PATH}\")\n",
        "print(f\"History path: {HISTORY_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9640a8e1-6fba-4b6d-8207-86ac87bde26b",
      "metadata": {
        "id": "9640a8e1-6fba-4b6d-8207-86ac87bde26b"
      },
      "source": [
        "### Inspect the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c5d817-c8e1-4fb8-a617-f95bf3702281",
      "metadata": {
        "id": "f7c5d817-c8e1-4fb8-a617-f95bf3702281"
      },
      "outputs": [],
      "source": [
        "### COMPLETE FOR HACKATHON\n",
        "def InspectFunction(X,labels,fileIdx,w = 8192):\n",
        "    pIdx = labels[0]\n",
        "    sIdx = labels[1]\n",
        "\n",
        "    plt.plot(X[:w])\n",
        "    plt.show()\n",
        "    # Plot A single seismogram...Continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OCBVpq0XTAyT",
      "metadata": {
        "id": "OCBVpq0XTAyT"
      },
      "outputs": [],
      "source": [
        "# Use automatically detected path\n",
        "dataDir = TRAIN_DIR\n",
        "print(f\"Using data directory: {dataDir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a084ff0-d706-4f41-9ef6-5ec6efe1c36b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9a084ff0-d706-4f41-9ef6-5ec6efe1c36b",
        "outputId": "a3e57649-dd68-406e-e4cc-381eea30bd2d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## List of files within the directory\n",
        "\n",
        "\n",
        "fileList = sorted(os.listdir(dataDir))\n",
        "\n",
        "## Read in n files (CHANGE ME)\n",
        "startFile = 0\n",
        "numFiles = 5\n",
        "w = 8192   # length of signal (Seismogram)\n",
        "\n",
        "## Iterate through files and plot with inspect_function()\n",
        "for fileIdx,fileName in enumerate(fileList[startFile:(startFile+numFiles)]):\n",
        "    ## Read in the file as a dataframe df\n",
        "    filePath = os.path.join(dataDir, fileName)\n",
        "    with gzip.open(filePath, 'rt') as file:\n",
        "        ## extract p and s labels\n",
        "        firstLine = file.readline().strip()\n",
        "        ## extract seismogram as colomns of x,y,z\n",
        "        df = pd.read_csv(file,header=None, engine='python')\n",
        "\n",
        "    ## Extract information from the dataframe\n",
        "    labels = np.array(firstLine.split(','), dtype=int)\n",
        "    X = df.iloc[0:, :3]\n",
        "\n",
        "    ## Inspect a single example usinf your plot_function\n",
        "    InspectFunction(X,labels,fileIdx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473e7c53-710d-4939-bbbf-95fa46132193",
      "metadata": {
        "id": "473e7c53-710d-4939-bbbf-95fa46132193"
      },
      "source": [
        "### Build a data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "669d0b86-32c4-4614-878f-30ad849073ee",
      "metadata": {
        "id": "669d0b86-32c4-4614-878f-30ad849073ee"
      },
      "outputs": [],
      "source": [
        "### Initialize a training data generator (CHOOSE PARAMETERS)\n",
        "batchSize = 1\n",
        "trainGen = DataGenerator(dataDir, batch_size = batchSize, max_files = 100) # \"max_files = None \" uses all the files\n",
        "# trainGen.total_len()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ff6813-384b-4ac8-807e-6b098b72f125",
      "metadata": {
        "id": "64ff6813-384b-4ac8-807e-6b098b72f125"
      },
      "source": [
        "### Inspect the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb0a1f8f-bfcb-4c19-96c6-92e8dbbda970",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "bb0a1f8f-bfcb-4c19-96c6-92e8dbbda970",
        "outputId": "e66fc4d4-680e-4a44-c52e-702f56759b9d"
      },
      "outputs": [],
      "source": [
        "for i in range(batchSize):\n",
        "    ### Plot the seismogram and labels\n",
        "    batchNumber = 2  # Batch number\n",
        "    fileNumber = i   # File number in batch (max is batch_size - 1)\n",
        "\n",
        "    singleSeismogramData = trainGen[batchNumber][0][fileNumber][0]\n",
        "    singleSeismogramLabel = trainGen[batchNumber][1][fileNumber][0]\n",
        "\n",
        "    plt.plot(singleSeismogramData, alpha = 0.5)\n",
        "    plt.plot(singleSeismogramLabel*20)    # Use the scalar for visual purposes\n",
        "    plt.xlim(0,8192)                      # Trim plot for visual inspection eg. 4096 instead of 8192\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac4cf69-f48d-4080-8897-7e3c8c9e072a",
      "metadata": {
        "id": "5ac4cf69-f48d-4080-8897-7e3c8c9e072a"
      },
      "outputs": [],
      "source": [
        "### BUILD a validation dataset\n",
        "valDir = VAL_DIR\n",
        "print(f\"Using validation directory: {valDir}\")\n",
        "\n",
        "# Create validation data generator\n",
        "valGen = DataGenerator(valDir, batch_size=batchSize, max_files=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db9e7c5-3200-4d49-a941-fe75f6ed2458",
      "metadata": {
        "id": "4db9e7c5-3200-4d49-a941-fe75f6ed2458"
      },
      "source": [
        "### Build a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0445772-5511-4885-9613-66ae7269423b",
      "metadata": {
        "id": "a0445772-5511-4885-9613-66ae7269423b"
      },
      "outputs": [],
      "source": [
        "### Build your custom model if you are feeling confident (You can skip this)\n",
        "def CustomModel(input_size=(1, 8192, 3)): # Keep the same output size.\n",
        "    inputs = Input(shape=input_size)\n",
        "\n",
        "    ##### Example:\n",
        "    filter_shape = (1, 7)\n",
        "    output = Conv2D(3, filter_shape, activation=\"relu\", padding=\"same\")(inputs) # The 3 is the number of channels, which for the final layer is 3\n",
        "    ##### replace this block\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e52cfb6-29fa-456f-b7b8-1a7ea3f83242",
      "metadata": {
        "id": "1e52cfb6-29fa-456f-b7b8-1a7ea3f83242"
      },
      "outputs": [],
      "source": [
        "# Inputs and Outputs: (1,8192,3)\n",
        "model = UNetModel()  # change to your own custom model, or keep default \"UNetModel()\"\n",
        "model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001, clipvalue = 1.0),\n",
        "            loss='mse'\n",
        "            # loss=KLDivergence()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f260560d",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Auto-check for existing model and load if available\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Check if model checkpoint exists\n",
        "if os.path.exists(MODEL_CHECKPOINT_PATH):\n",
        "    print(\"Found existing model checkpoint. Loading...\")\n",
        "    model = load_model(MODEL_CHECKPOINT_PATH)\n",
        "    print(\"Model loaded successfully!\")\n",
        "    \n",
        "    # Load training history if it exists\n",
        "    if os.path.exists(HISTORY_PATH):\n",
        "        print(\"Found existing training history. Loading...\")\n",
        "        with open(HISTORY_PATH, 'rb') as f:\n",
        "            previous_history = pickle.load(f)\n",
        "        print(\"Previous training history loaded!\")\n",
        "        print(f\"Previous best val_loss: {min(previous_history.get('val_loss', [float('inf')]))}\")\n",
        "    else:\n",
        "        previous_history = None\n",
        "        print(\"No previous training history found.\")\n",
        "        \n",
        "else:\n",
        "    print(\"No existing model checkpoint found. Will start training from scratch.\")\n",
        "    previous_history = None\n",
        "\n",
        "print(f\"Model summary:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3603fa95-49a3-433b-90e5-86a76dc408db",
      "metadata": {
        "id": "3603fa95-49a3-433b-90e5-86a76dc408db"
      },
      "source": [
        "### Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zk7VOKLIWX51",
      "metadata": {
        "id": "zk7VOKLIWX51"
      },
      "source": [
        "##✅ Step 1: Save model weights and training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IwoLH8gYVqHf",
      "metadata": {
        "id": "IwoLH8gYVqHf"
      },
      "outputs": [],
      "source": [
        "# Add checkpoint using automatic path detection\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(MODEL_CHECKPOINT_PATH,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)\n",
        "\n",
        "print(f\"Model checkpoint will be saved to: {MODEL_CHECKPOINT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ce-nnV5KWgij",
      "metadata": {
        "id": "Ce-nnV5KWgij"
      },
      "source": [
        "##✅ Step 2: Save the training history object manually"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AWe8qdNrW49x",
      "metadata": {
        "id": "AWe8qdNrW49x"
      },
      "source": [
        "###Initial Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ba48fa-8617-46be-9528-10440e26912b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52ba48fa-8617-46be-9528-10440e26912b",
        "outputId": "33c5b651-ebaf-4e97-e866-0319f1e822c5"
      },
      "outputs": [],
      "source": [
        "# Training with proper validation data generator\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    trainGen,\n",
        "    validation_data=valGen,  # Use proper validation data\n",
        "    epochs=10,\n",
        "    callbacks=[reduce_lr, early_stop, checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IS7ZlNXEXIk6",
      "metadata": {
        "id": "IS7ZlNXEXIk6"
      },
      "source": [
        "###Continue Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X50GFOq3XQ6C",
      "metadata": {
        "id": "X50GFOq3XQ6C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "HISTORY_PATH = 'training_history.pkl'\n",
        "\n",
        "# Load previous training history if it exists (for continue training)\n",
        "if os.path.exists(HISTORY_PATH):\n",
        "    with open(HISTORY_PATH, 'rb') as f:\n",
        "        previous_history = pickle.load(f)\n",
        "    print(\"Previous training history loaded!\")\n",
        "    print(f\"Previous best val_loss: {min(previous_history.get('val_loss', [float('inf')])):.6f}\")\n",
        "    print(f\"Total previous epochs: {len(previous_history.get('loss', []))}\")\n",
        "else:\n",
        "    print(\"No previous training history found.\")\n",
        "    previous_history = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Y3HRfyhXLha",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y3HRfyhXLha",
        "outputId": "cf552218-aef0-45f4-8c72-062777a9c1b6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define your model checkpoint path\n",
        "MODEL_CHECKPOINT_PATH = \"model_checkpoint.keras\"\n",
        "\n",
        "# Continue training - load model automatically if checkpoint exists\n",
        "if os.path.exists(MODEL_CHECKPOINT_PATH):\n",
        "    print(\"Loading model from checkpoint...\")\n",
        "    model = load_model(MODEL_CHECKPOINT_PATH)\n",
        "    print(\"✅ Model loaded successfully for continued training!\")\n",
        "else:\n",
        "    print(\"⚠️ No checkpoint found. Make sure to run initial training first.\")\n",
        "\n",
        "# Set up callbacks for continued training\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "checkpoint = ModelCheckpoint(MODEL_CHECKPOINT_PATH, save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Continue training with more epochs\n",
        "print(\"Starting continued training...\")\n",
        "history = model.fit(\n",
        "    trainGen,\n",
        "    validation_data=valGen,  # Use proper validation data\n",
        "    epochs=10,               # continue with more epochs\n",
        "    callbacks=[reduce_lr, early_stop, checkpoint]\n",
        ")\n",
        "\n",
        "print(\"✅ Continued training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hyi7eFFPWyXm",
      "metadata": {
        "id": "Hyi7eFFPWyXm"
      },
      "source": [
        "###After training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a017675",
      "metadata": {},
      "source": [
        "### 🎯 Training Workflow with Smart Model Saving\n",
        "\n",
        "**New Features:**\n",
        "1. **Performance Comparison**: Automatically compares current training with previous training history\n",
        "2. **Visual Analysis**: Shows side-by-side plots of current vs previous training performance\n",
        "3. **Improvement Detection**: Calculates whether the model improved and by how much\n",
        "4. **Interactive Decision**: Lets you choose whether to save the model based on performance\n",
        "5. **Automatic Backup**: Creates timestamped backups when saving\n",
        "6. **Smart Restoration**: Loads previous best model if you choose not to save\n",
        "\n",
        "**Workflow:**\n",
        "1. Train your model (initial or continued training)\n",
        "2. Review the performance comparison charts\n",
        "3. Check the improvement analysis\n",
        "4. Decide whether to save the model\n",
        "5. If saved: New model becomes the checkpoint\n",
        "6. If not saved: Previous best model is restored\n",
        "\n",
        "**Recommendation Logic:**\n",
        "- ✅ **Save** if validation loss improved\n",
        "- ❌ **Don't save** if validation loss got worse\n",
        "- 🤔 **Your choice** for mixed results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61eiBWK0WxmN",
      "metadata": {
        "id": "61eiBWK0WxmN"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Save training history using automatic path\n",
        "def save_history(history, previous_history=None):\n",
        "    \"\"\"Save training history, optionally combining with previous history\"\"\"\n",
        "    if previous_history is not None:\n",
        "        # Combine with previous history\n",
        "        combined_history = {}\n",
        "        for key in history.history.keys():\n",
        "            if key in previous_history:\n",
        "                combined_history[key] = previous_history[key] + history.history[key]\n",
        "            else:\n",
        "                combined_history[key] = history.history[key]\n",
        "        \n",
        "        # Add any keys that were only in previous history\n",
        "        for key in previous_history.keys():\n",
        "            if key not in combined_history:\n",
        "                combined_history[key] = previous_history[key]\n",
        "    else:\n",
        "        combined_history = history.history\n",
        "    \n",
        "    # Save combined history\n",
        "    with open(HISTORY_PATH, 'wb') as f:\n",
        "        pickle.dump(combined_history, f)\n",
        "    \n",
        "    print(f\"Training history saved to: {HISTORY_PATH}\")\n",
        "    return combined_history\n",
        "\n",
        "# Training History Comparison and Model Saving Decision\n",
        "def compare_training_performance(current_history, previous_history=None):\n",
        "    \"\"\"Compare current training performance with previous training history\"\"\"\n",
        "    \n",
        "    # Create subplots for comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Training Performance Comparison', fontsize=16)\n",
        "    \n",
        "    # Current training metrics\n",
        "    current_loss = current_history.history['loss']\n",
        "    current_val_loss = current_history.history.get('val_loss', [])\n",
        "    \n",
        "    # Plot current training loss\n",
        "    axes[0, 0].plot(current_loss, 'b-', label='Current Training Loss', linewidth=2)\n",
        "    if current_val_loss:\n",
        "        axes[0, 0].plot(current_val_loss, 'r-', label='Current Validation Loss', linewidth=2)\n",
        "    axes[0, 0].set_title('Current Training Session')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Performance metrics\n",
        "    current_best_val_loss = min(current_val_loss) if current_val_loss else float('inf')\n",
        "    current_final_loss = current_loss[-1] if current_loss else float('inf')\n",
        "    \n",
        "    improvement_text = f\"Current Session:\\n\"\n",
        "    improvement_text += f\"Final Loss: {current_final_loss:.6f}\\n\"\n",
        "    improvement_text += f\"Best Val Loss: {current_best_val_loss:.6f}\\n\"\n",
        "    improvement_text += f\"Epochs: {len(current_loss)}\\n\"\n",
        "    \n",
        "    if previous_history is not None:\n",
        "        # Previous training metrics\n",
        "        prev_loss = previous_history.get('loss', [])\n",
        "        prev_val_loss = previous_history.get('val_loss', [])\n",
        "        \n",
        "        # Plot previous training loss\n",
        "        axes[0, 1].plot(prev_loss, 'g-', label='Previous Training Loss', linewidth=2)\n",
        "        if prev_val_loss:\n",
        "            axes[0, 1].plot(prev_val_loss, 'orange', label='Previous Validation Loss', linewidth=2)\n",
        "        axes[0, 1].set_title('Previous Training History')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Compare performance\n",
        "        prev_best_val_loss = min(prev_val_loss) if prev_val_loss else float('inf')\n",
        "        prev_final_loss = prev_loss[-1] if prev_loss else float('inf')\n",
        "        \n",
        "        improvement_text += f\"\\nPrevious Best:\\n\"\n",
        "        improvement_text += f\"Final Loss: {prev_final_loss:.6f}\\n\"\n",
        "        improvement_text += f\"Best Val Loss: {prev_best_val_loss:.6f}\\n\"\n",
        "        improvement_text += f\"Total Epochs: {len(prev_loss)}\\n\"\n",
        "        \n",
        "        # Determine if there's improvement\n",
        "        val_loss_improved = current_best_val_loss < prev_best_val_loss\n",
        "        loss_improved = current_final_loss < prev_final_loss\n",
        "        \n",
        "        improvement_text += f\"\\n📊 IMPROVEMENT ANALYSIS:\\n\"\n",
        "        improvement_text += f\"Val Loss: {'✅ IMPROVED' if val_loss_improved else '❌ WORSE'} \"\n",
        "        improvement_text += f\"({prev_best_val_loss:.6f} → {current_best_val_loss:.6f})\\n\"\n",
        "        improvement_text += f\"Final Loss: {'✅ IMPROVED' if loss_improved else '❌ WORSE'} \"\n",
        "        improvement_text += f\"({prev_final_loss:.6f} → {current_final_loss:.6f})\\n\"\n",
        "        \n",
        "        # Overall recommendation\n",
        "        overall_improved = val_loss_improved or loss_improved\n",
        "        recommendation = \"💾 RECOMMEND SAVING\" if overall_improved else \"⚠️ CONSIDER NOT SAVING\"\n",
        "        improvement_text += f\"\\n🎯 {recommendation}\"\n",
        "        \n",
        "        # Combined plot\n",
        "        axes[1, 0].plot(range(len(prev_loss)), prev_loss, 'g-', label='Previous Training', linewidth=2)\n",
        "        axes[1, 0].plot(range(len(prev_loss), len(prev_loss) + len(current_loss)), \n",
        "                       current_loss, 'b-', label='Current Training', linewidth=2)\n",
        "        if prev_val_loss and current_val_loss:\n",
        "            axes[1, 0].plot(range(len(prev_val_loss)), prev_val_loss, 'orange', \n",
        "                           label='Previous Validation', linewidth=2)\n",
        "            axes[1, 0].plot(range(len(prev_val_loss), len(prev_val_loss) + len(current_val_loss)), \n",
        "                           current_val_loss, 'r-', label='Current Validation', linewidth=2)\n",
        "        axes[1, 0].axvline(x=len(prev_loss), color='black', linestyle='--', alpha=0.5, label='Training Resumed')\n",
        "        axes[1, 0].set_title('Complete Training History')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "    else:\n",
        "        axes[0, 1].text(0.5, 0.5, 'No Previous Training History', \n",
        "                       horizontalalignment='center', verticalalignment='center', \n",
        "                       transform=axes[0, 1].transAxes, fontsize=14)\n",
        "        axes[0, 1].set_title('Previous Training History')\n",
        "        axes[1, 0].plot(current_loss, 'b-', label='Current Training Loss', linewidth=2)\n",
        "        if current_val_loss:\n",
        "            axes[1, 0].plot(current_val_loss, 'r-', label='Current Validation Loss', linewidth=2)\n",
        "        axes[1, 0].set_title('Training History')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        improvement_text += f\"\\n🎯 💾 RECOMMEND SAVING (First Training)\"\n",
        "    \n",
        "    # Display improvement text\n",
        "    axes[1, 1].text(0.05, 0.95, improvement_text, transform=axes[1, 1].transAxes, \n",
        "                   fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n",
        "    axes[1, 1].set_xlim(0, 1)\n",
        "    axes[1, 1].set_ylim(0, 1)\n",
        "    axes[1, 1].axis('off')\n",
        "    axes[1, 1].set_title('Performance Summary')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return current_best_val_loss, previous_history.get('val_loss', [float('inf')])[-1] if previous_history and previous_history.get('val_loss') else float('inf')\n",
        "\n",
        "# Save the current training history\n",
        "final_history = save_history(history, previous_history)\n",
        "print(f\"Total epochs trained: {len(final_history['loss'])}\")\n",
        "print(f\"Best val_loss: {min(final_history.get('val_loss', [float('inf')]))}\")\n",
        "\n",
        "# Compare performance\n",
        "current_best_val_loss, previous_best_val_loss = compare_training_performance(history, previous_history)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"🎯 TRAINING PERFORMANCE ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6868b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Model Saving Decision\n",
        "def save_model_decision(current_history, previous_history=None):\n",
        "    \"\"\"Allow user to decide whether to save the model based on performance\"\"\"\n",
        "    \n",
        "    # Calculate improvement metrics\n",
        "    current_val_loss = current_history.history.get('val_loss', [])\n",
        "    current_best_val_loss = min(current_val_loss) if current_val_loss else float('inf')\n",
        "    \n",
        "    if previous_history and previous_history.get('val_loss'):\n",
        "        prev_best_val_loss = min(previous_history.get('val_loss', [float('inf')]))\n",
        "        improved = current_best_val_loss < prev_best_val_loss\n",
        "        improvement_pct = ((prev_best_val_loss - current_best_val_loss) / prev_best_val_loss) * 100\n",
        "    else:\n",
        "        improved = True  # First training session\n",
        "        improvement_pct = 0\n",
        "        prev_best_val_loss = float('inf')\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🤖 MODEL SAVING DECISION SYSTEM\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if improved:\n",
        "        print(\"✅ PERFORMANCE IMPROVED!\")\n",
        "        if improvement_pct > 0:\n",
        "            print(f\"📈 Validation Loss Improved by {improvement_pct:.2f}%\")\n",
        "        print(f\"🎯 Previous Best Val Loss: {prev_best_val_loss:.6f}\")\n",
        "        print(f\"🚀 Current Best Val Loss: {current_best_val_loss:.6f}\")\n",
        "        print(\"💡 Recommendation: SAVE THE MODEL\")\n",
        "    else:\n",
        "        print(\"❌ PERFORMANCE DID NOT IMPROVE\")\n",
        "        print(f\"📉 Validation Loss Got Worse by {abs(improvement_pct):.2f}%\")\n",
        "        print(f\"🎯 Previous Best Val Loss: {prev_best_val_loss:.6f}\")\n",
        "        print(f\"📉 Current Best Val Loss: {current_best_val_loss:.6f}\")\n",
        "        print(\"💡 Recommendation: DO NOT SAVE THE MODEL\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    \n",
        "    # Get user decision\n",
        "    while True:\n",
        "        decision = input(\"Do you want to save this model? (y/n): \").lower().strip()\n",
        "        if decision in ['y', 'yes']:\n",
        "            return True\n",
        "        elif decision in ['n', 'no']:\n",
        "            return False\n",
        "        else:\n",
        "            print(\"Please enter 'y' for yes or 'n' for no.\")\n",
        "\n",
        "# Get user decision\n",
        "save_model = save_model_decision(history, previous_history)\n",
        "\n",
        "if save_model:\n",
        "    print(\"\\n💾 SAVING MODEL AND TRAINING HISTORY...\")\n",
        "    \n",
        "    # Save the model (it's already saved by checkpoint, but let's make sure)\n",
        "    model.save(MODEL_CHECKPOINT_PATH)\n",
        "    print(f\"✅ Model saved to: {MODEL_CHECKPOINT_PATH}\")\n",
        "    \n",
        "    # Combine and save training history\n",
        "    if previous_history is not None:\n",
        "        combined_history = {}\n",
        "        for key in history.history.keys():\n",
        "            if key in previous_history:\n",
        "                combined_history[key] = previous_history[key] + history.history[key]\n",
        "            else:\n",
        "                combined_history[key] = history.history[key]\n",
        "        \n",
        "        # Add any keys that were only in previous history\n",
        "        for key in previous_history.keys():\n",
        "            if key not in combined_history:\n",
        "                combined_history[key] = previous_history[key]\n",
        "    else:\n",
        "        combined_history = history.history\n",
        "    \n",
        "    # Save combined history\n",
        "    with open(HISTORY_PATH, 'wb') as f:\n",
        "        pickle.dump(combined_history, f)\n",
        "    \n",
        "    print(f\"✅ Training history saved to: {HISTORY_PATH}\")\n",
        "    print(f\"📊 Total epochs in history: {len(combined_history['loss'])}\")\n",
        "    print(f\"🎯 Best validation loss: {min(combined_history.get('val_loss', [float('inf')])):.6f}\")\n",
        "    \n",
        "    # Save a backup with timestamp\n",
        "    import datetime\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_model_path = MODEL_CHECKPOINT_PATH.replace('.keras', f'_backup_{timestamp}.keras')\n",
        "    backup_history_path = HISTORY_PATH.replace('.pkl', f'_backup_{timestamp}.pkl')\n",
        "    \n",
        "    model.save(backup_model_path)\n",
        "    with open(backup_history_path, 'wb') as f:\n",
        "        pickle.dump(combined_history, f)\n",
        "    \n",
        "    print(f\"💾 Backup saved: {backup_model_path}\")\n",
        "    print(f\"💾 Backup history: {backup_history_path}\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\n❌ MODEL NOT SAVED\")\n",
        "    print(\"The current model weights and training history will not be saved.\")\n",
        "    print(\"Previous best model remains unchanged.\")\n",
        "    if previous_history:\n",
        "        print(f\"Previous best validation loss: {min(previous_history.get('val_loss', [float('inf')])):.6f}\")\n",
        "    \n",
        "    # Load back the previous best model if it exists\n",
        "    if os.path.exists(MODEL_CHECKPOINT_PATH):\n",
        "        print(\"🔄 Loading previous best model...\")\n",
        "        model = load_model(MODEL_CHECKPOINT_PATH)\n",
        "        print(\"✅ Previous best model loaded.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 TRAINING SESSION COMPLETE!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad2b411-e084-4164-9c25-8f65f09dbc83",
      "metadata": {
        "id": "bad2b411-e084-4164-9c25-8f65f09dbc83"
      },
      "source": [
        "### Analyse training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa5dc3ec-13d1-47bd-a8c8-2b101cedf2f4",
      "metadata": {
        "id": "fa5dc3ec-13d1-47bd-a8c8-2b101cedf2f4"
      },
      "outputs": [],
      "source": [
        "# Analyze training history\n",
        "# Use combined history if available, otherwise use current history\n",
        "if 'combined_history' in locals():\n",
        "    loss = combined_history['loss']\n",
        "    val_loss = combined_history.get('val_loss', [])\n",
        "    print(f\"Analyzing combined training history with {len(loss)} epochs\")\n",
        "else:\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history.get('val_loss', [])\n",
        "    print(f\"Analyzing current training history with {len(loss)} epochs\")\n",
        "\n",
        "print(f\"Final training loss: {loss[-1]:.6f}\")\n",
        "if val_loss:\n",
        "    print(f\"Final validation loss: {val_loss[-1]:.6f}\")\n",
        "    print(f\"Best validation loss: {min(val_loss):.6f}\")\n",
        "    print(f\"Best validation loss achieved at epoch: {val_loss.index(min(val_loss)) + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b82a4f-8349-42e4-8d35-022c54a2cb40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "95b82a4f-8349-42e4-8d35-022c54a2cb40",
        "outputId": "b12b9abd-cb7d-4f00-d4e7-2741e1b3a692"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'loss' and 'val_loss' are already defined\n",
        "# loss = [...]  # Training loss values\n",
        "# val_loss = [...]  # Validation loss values (if available)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss, 'b-', label='Training Loss', linewidth=2)\n",
        "if val_loss:\n",
        "    plt.plot(val_loss, 'r-', label='Validation Loss', linewidth=2)\n",
        "    # Mark the best validation loss\n",
        "    best_val_epoch = val_loss.index(min(val_loss))\n",
        "    plt.plot(best_val_epoch, min(val_loss), 'ro', markersize=10, label=f'Best Val Loss: {min(val_loss):.6f}')\n",
        "plt.title('Training Progress')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot loss improvement over time\n",
        "plt.subplot(1, 2, 2)\n",
        "if val_loss:\n",
        "    # Calculate moving average for smoother visualization\n",
        "    window_size = min(5, len(val_loss))\n",
        "    if window_size > 1:\n",
        "        val_loss_smooth = []\n",
        "        for i in range(len(val_loss)):\n",
        "            start_idx = max(0, i - window_size + 1)\n",
        "            val_loss_smooth.append(np.mean(val_loss[start_idx:i+1]))\n",
        "        plt.plot(val_loss_smooth, 'g-', label=f'Validation Loss (Moving Avg)', linewidth=2)\n",
        "    plt.plot(val_loss, 'r-', alpha=0.5, label='Validation Loss', linewidth=1)\n",
        "    plt.title('Validation Loss Trend')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'No validation loss data available', \n",
        "             horizontalalignment='center', verticalalignment='center', \n",
        "             transform=plt.gca().transAxes, fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df59c50e-5807-46fe-9d9e-5709b4d706eb",
      "metadata": {
        "id": "df59c50e-5807-46fe-9d9e-5709b4d706eb"
      },
      "source": [
        "### Predict on a test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f23fddb-85f4-4b37-aeb0-fba4893558c3",
      "metadata": {
        "id": "2f23fddb-85f4-4b37-aeb0-fba4893558c3"
      },
      "outputs": [],
      "source": [
        "# Use automatically detected test directory\n",
        "testDir = TEST_DIR\n",
        "\n",
        "# This is a challenge testSet of 6 seismograms\n",
        "challengeDir = CHALLENGE_DIR\n",
        "\n",
        "print(f\"Test directory: {testDir}\")\n",
        "print(f\"Challenge directory: {challengeDir}\")\n",
        "\n",
        "# Choose which directory to use for testing\n",
        "# testDir = challengeDir  # Uncomment this line to use challenge data instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fca68de-b8db-435e-976d-9c34fb7817e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fca68de-b8db-435e-976d-9c34fb7817e5",
        "outputId": "e1d18ae2-115b-460a-c9d5-c29eac6b8565"
      },
      "outputs": [],
      "source": [
        "### The training dataset \"trainGen\" is used here. USE YOUR testGenerator INSTEAD.\n",
        "# Create test data generator\n",
        "testGen = DataGenerator(testDir, batch_size=batchSize, max_files=None)  # Use all test files\n",
        "print(f\"Test generator created with {testGen.total_len()} samples\")\n",
        "\n",
        "# Make predictions on test data\n",
        "Predictions = model.predict(testGen)\n",
        "print(f\"Predictions shape: {Predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "175215b9-4503-42e8-ab55-40a429c4ace6",
      "metadata": {
        "id": "175215b9-4503-42e8-ab55-40a429c4ace6"
      },
      "source": [
        "### Analyse predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04e0c51-dbf4-4b33-a613-915a2b640f60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f04e0c51-dbf4-4b33-a613-915a2b640f60",
        "outputId": "bd353402-608f-4bd3-ac8f-90426731faec"
      },
      "outputs": [],
      "source": [
        "print(Predictions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40dc497e-66b0-4781-a808-b03294b6eab9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "40dc497e-66b0-4781-a808-b03294b6eab9",
        "outputId": "f34451e5-1e59-4579-c86f-e8a0b3dc4e1c"
      },
      "outputs": [],
      "source": [
        "for seismogramNum in range(5):\n",
        "    plt.plot(Predictions[seismogramNum][0][:,:2])\n",
        "    plt.xlim()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c89fc559-f0ce-4242-ae9f-16cc8cd010d3",
      "metadata": {
        "id": "c89fc559-f0ce-4242-ae9f-16cc8cd010d3"
      },
      "source": [
        "### Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde9ae87-7cbd-4bc7-acc1-acb11b86298f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "fde9ae87-7cbd-4bc7-acc1-acb11b86298f",
        "outputId": "875ddba3-c721-440e-d8bc-03da1f0fc92b"
      },
      "outputs": [],
      "source": [
        "# Model Performance Evaluation\n",
        "# Negative means the Pick is after the label (Late) and Positive is Early\n",
        "try:\n",
        "    fig = resultsHistogram(Preds=Predictions, dataGen=testGen)\n",
        "    print(\"Performance histogram generated successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error generating histogram: {e}\")\n",
        "    print(\"Make sure the resultsHistogram function is available in Utils.ipynb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a0456e-fa50-40e9-bfe1-df936ca83140",
      "metadata": {
        "id": "78a0456e-fa50-40e9-bfe1-df936ca83140"
      },
      "outputs": [],
      "source": [
        "### Still feeling curious?\n",
        "# Explore the Utils notebook to customise further. Remember to save the Utils notebook to a new checkpoint\n",
        "# and to run the first cell of this notebook: or run the command \" %run ./Utils.ipynb \" in a cell to update your changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf34bc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Summary\n",
        "print(\"=\" * 60)\n",
        "print(\"ENVIRONMENT CONFIGURATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
        "print(f\"Training directory: {TRAIN_DIR}\")\n",
        "print(f\"Validation directory: {VAL_DIR}\")\n",
        "print(f\"Test directory: {TEST_DIR}\")\n",
        "print(f\"Challenge directory: {CHALLENGE_DIR}\")\n",
        "print(f\"Model checkpoint: {MODEL_CHECKPOINT_PATH}\")\n",
        "print(f\"Training history: {HISTORY_PATH}\")\n",
        "print(f\"Model exists: {os.path.exists(MODEL_CHECKPOINT_PATH)}\")\n",
        "print(f\"History exists: {os.path.exists(HISTORY_PATH)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Show model summary if model exists\n",
        "if 'model' in locals():\n",
        "    print(\"\\nModel Summary:\")\n",
        "    model.summary()\n",
        "else:\n",
        "    print(\"\\nNo model loaded yet. Run the model creation cells first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
